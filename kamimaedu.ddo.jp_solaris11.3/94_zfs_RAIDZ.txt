goran (Oracle VM + Oracle Solaris 11 11/11 x86)

ZFS で RAID-Z を構成する

1. 状態確認

   1. 物理ディスクの状況
   # format
   AVAILABLE DISK SELECTIONS:
          0. c3t0d0 <ATA-VBOX HARDDISK-1.0 cyl 5218 alt 2 hd 255 sec 63>
             /pci@0,0/pci8086,2829@d/disk@0,0
          1. c3t1d0 <ATA-VBOX HARDDISK-1.0 cyl 5218 alt 2 hd 255 sec 63>
             /pci@0,0/pci8086,2829@d/disk@1,0
          2. c3t2d0 <ATA-VBOX HARDDISK-1.0 cyl 5218 alt 2 hd 255 sec 63>
             /pci@0,0/pci8086,2829@d/disk@2,0
          3. c3t3d0 <ATA-VBOX HARDDISK-1.0 cyl 1022 alt 2 hd 64 sec 32>
             /pci@0,0/pci8086,2829@d/disk@3,0
          4. c3t4d0 <ATA-VBOX HARDDISK-1.0 cyl 1022 alt 2 hd 64 sec 32>
             /pci@0,0/pci8086,2829@d/disk@4,0
          5. c3t5d0 <ATA-VBOX HARDDISK-1.0 cyl 1022 alt 2 hd 64 sec 32>
             /pci@0,0/pci8086,2829@d/disk@5,0
   
   c3t3d0 (1.0GB)、c3t4d0 (1.0GB)、c3t5d0 (1.0GB) を利用し RAIDZ を構成します

   2. ZFS 状態確認
   # zpool status
     pool: rpool
    state: ONLINE
     scan: resilvered 19.9G in 0h38m with 0 errors on Fri Sep 28 17:44:38 2012
   config:
           NAME          STATE     READ WRITE CKSUM
           rpool         ONLINE       0     0     0
             mirror-0    ONLINE       0     0     0
               c3t0d0s0  ONLINE       0     0     0
               c3t1d0s0  ONLINE       0     0     0
           spares
             c3t2d0s0    AVAIL

   # zfs list -t all
   NAME                     USED  AVAIL  REFER  MOUNTPOINT
   rpool                   20.1G  19.0G    39K  /rpool
   rpool/ROOT              17.9G  19.0G    31K  legacy
   rpool/ROOT/solaris      17.9G  19.0G  16.5G  /
   rpool/ROOT/solaris/var  1.42G  19.0G  1.42G  /var
   rpool/dump              1.03G  19.1G  1.00G  -
   rpool/export             100M  19.0G    32K  /export
   rpool/export/home        100M  19.0G   100M  /export/home
   rpool/swap              1.03G  19.1G  1.00G  -

2. RAID-Z ストレージプールの作成

   1. RAID-Z ストレージプールの作成
   # zpool create -f zpool1 raidz c3t3d0 c3t4d0 c3t5d0

   2. 確認
   # zpool status
     pool: zpool1
    state: ONLINE
     scan: none requested
   config:
           NAME        STATE     READ WRITE CKSUM
           zpool1      ONLINE       0     0     0
             raidz1-0  ONLINE       0     0     0
               c3t3d0  ONLINE       0     0     0
               c3t4d0  ONLINE       0     0     0
               c3t5d0  ONLINE       0     0     0

   # zpool list
   NAME     SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
   rpool   39.8G  20.0G  19.7G  50%  1.00x  ONLINE  -
   zpool1  2.95G   176K  2.95G   0%  1.00x  ONLINE  -

   # zfs list
   NAME                     USED  AVAIL  REFER  MOUNTPOINT
   rpool                   20.1G  19.0G    39K  /rpool
   rpool/ROOT              17.9G  19.0G    31K  legacy
   rpool/ROOT/solaris      17.9G  19.0G  16.5G  /
   rpool/ROOT/solaris/var  1.42G  19.0G  1.42G  /var
   rpool/dump              1.03G  19.1G  1.00G  -
   rpool/export             100M  19.0G    32K  /export
   rpool/export/home        100M  19.0G   100M  /export/home
   rpool/swap              1.03G  19.1G  1.00G  -
   zpool1                   117K  1.94G  34.6K  /zpool1

3. ファイルシステムの作成

   1. ファイルシステムの作成
   # zfs create zpool1/data1

   2. 重複排除機能の有効化
   # zfs dedup=on zpool1/data1
   
   debup の指定には on / verify の二種類が存在する
   ZFS はツリー全体で各ブロックでチェックサムを計算することで非常に高い整合性を実現している

   on の場合、ハッシュ単位 (SHA 256) で比較するため、非常に低い確率ですが不整合が発生する可能性があります
   verify の場合、ハッシュ単位比較 + バイト単位比較 を行うため、性能は落ちるものの整合性は高いです

   今回は on を選択します

   3. 圧縮機能の有効化
   # zfs compress=on zpool1/data1

   4. マウントポイントの変更
   # zfs mountpoint=/data1 zpool1/data1

   5. 確認
   # zfs get all zpool1/data1
   NAME          PROPERTY              VALUE                    SOURCE
   zpool1/data1  type                  filesystem               -
   zpool1/data1  creation              月 10月  1 15:55 2012  -
   zpool1/data1  used                  34.6K                    -
   zpool1/data1  available             1.94G                    -
   zpool1/data1  referenced            34.6K                    -
   zpool1/data1  compressratio         1.00x                    -
   zpool1/data1  mounted               yes                      -
   zpool1/data1  quota                 none                     default
   zpool1/data1  reservation           none                     default
   zpool1/data1  recordsize            128K                     default
   zpool1/data1  mountpoint            /data1                   local
   zpool1/data1  sharenfs              off                      default
   zpool1/data1  checksum              on                       default
   zpool1/data1  compression           on                       local
   zpool1/data1  atime                 on                       default
   zpool1/data1  devices               on                       default
   zpool1/data1  exec                  on                       default
   zpool1/data1  setuid                on                       default
   zpool1/data1  readonly              off                      default
   zpool1/data1  zoned                 off                      default
   zpool1/data1  snapdir               hidden                   default
   zpool1/data1  aclmode               discard                  default
   zpool1/data1  aclinherit            restricted               default
   zpool1/data1  canmount              on                       default
   zpool1/data1  xattr                 on                       default
   zpool1/data1  copies                1                        default
   zpool1/data1  version               5                        -
   zpool1/data1  utf8only              off                      -
   zpool1/data1  normalization         none                     -
   zpool1/data1  casesensitivity       mixed                    -
   zpool1/data1  vscan                 off                      default
   zpool1/data1  nbmand                off                      default
   zpool1/data1  sharesmb              off                      default
   zpool1/data1  refquota              none                     default
   zpool1/data1  refreservation        none                     default
   zpool1/data1  primarycache          all                      default
   zpool1/data1  secondarycache        all                      default
   zpool1/data1  usedbysnapshots       0                        -
   zpool1/data1  usedbydataset         34.6K                    -
   zpool1/data1  usedbychildren        0                        -
   zpool1/data1  usedbyrefreservation  0                        -
   zpool1/data1  logbias               latency                  default
   zpool1/data1  dedup                 on                       local
   zpool1/data1  mlslabel              none                     -
   zpool1/data1  sync                  standard                 default
   zpool1/data1  encryption            off                      -
   zpool1/data1  keysource             none                     default
   zpool1/data1  keystatus             none                     -
   zpool1/data1  rekeydate             -                        default
   zpool1/data1  rstchown              on                       default
   zpool1/data1  shadow                none                     -




